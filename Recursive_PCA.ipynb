{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class RecursivePCA:\n",
    "    def __init__(self, input_size, output_size, num_layers):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.pcas = []\n",
    "\n",
    "        # Calculate the size for each layer\n",
    "        difference = input_size - output_size\n",
    "        avg_step = difference // num_layers\n",
    "        extra = difference % num_layers\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            n_components = input_size - avg_step\n",
    "            if extra > 0:\n",
    "                n_components -= 1\n",
    "                extra -= 1\n",
    "            n_components = max(n_components, output_size)  # Ensure we don't go below the output size\n",
    "            self.pcas.append(PCA(n_components=n_components))\n",
    "            input_size = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        temp_data = X\n",
    "        for pca in self.pcas:\n",
    "            temp_data = pca.fit_transform(temp_data)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        temp_data = X\n",
    "        for pca in self.pcas:\n",
    "            temp_data = pca.transform(temp_data)\n",
    "        return temp_data\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        temp_data = X\n",
    "        for pca in reversed(self.pcas):\n",
    "            temp_data = pca.inverse_transform(temp_data)\n",
    "            if len(temp_data.shape) == 3:\n",
    "                temp_data = temp_data.squeeze(0)\n",
    "        return temp_data\n",
    "    \n",
    "    def save(self, dir_name):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "            \n",
    "        for i, pca in enumerate(self.pcas):\n",
    "            with open(os.path.join(dir_name, f'pca_layer_{i}.pkl'), 'wb') as f:\n",
    "                pickle.dump(pca, f)\n",
    "                \n",
    "    def load(self, dir_name):\n",
    "        self.pcas = []\n",
    "        for i in range(self.num_layers):\n",
    "            with open(os.path.join(dir_name, f'pca_layer_{i}.pkl'), 'rb') as f:\n",
    "                pca = pickle.load(f)\n",
    "                self.pcas.append(pca)\n",
    "    \n",
    "    def compute_loss_at_each_layer(self, data):\n",
    "        temp_data = data\n",
    "        losses = []\n",
    "\n",
    "        for pca in self.pcas:\n",
    "            transformed_data = pca.transform(temp_data)\n",
    "            cumulative_explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "            loss = 1 - cumulative_explained_variance\n",
    "            losses.append(loss)\n",
    "            temp_data = transformed_data\n",
    "\n",
    "        return losses\n",
    "    def components_at_each_layer(self):\n",
    "        return [pca.n_components_ for pca in self.pcas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "Reconstruction MSE: 0.03442259877920151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize the data to be in the range [0,1]\n",
    "train_images = train_images.reshape(train_images.shape[0], 28*28).astype('float32') / 255\n",
    "test_images = test_images.reshape(test_images.shape[0], 28*28).astype('float32') / 255\n",
    "\n",
    "# Create an instance of RecursivePCA\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "num_layers = 200\n",
    "r_pca = RecursivePCA(input_size, output_size, num_layers)\n",
    "\n",
    "\n",
    "# Fit on training data\n",
    "r_pca.fit(train_images)\n",
    "\n",
    "# Transform the data\n",
    "compressed_train_data = r_pca.transform(train_images)\n",
    "\n",
    "# Print the shape to see the compressed size\n",
    "print(compressed_train_data.shape)  # This should show (60000, 2)\n",
    "\n",
    "# Inverse Transform the data\n",
    "reconstructed_train_data = r_pca.inverse_transform(compressed_train_data)\n",
    "\n",
    "# Let's see the reconstruction error (MSE)\n",
    "mse = np.mean((train_images - reconstructed_train_data)**2)\n",
    "print(f\"Reconstruction MSE: {mse}\")\n",
    "\n",
    "# Save the PCA models\n",
    "r_pca.save(\"pca_dir\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To demonstrate loading, we'll load the PCA models back\n",
    "loaded_r_pca = RecursivePCA(input_size, output_size, num_layers)\n",
    "loaded_r_pca.load(\"pca_dir\")\n",
    "\n",
    "# Transform using loaded PCA\n",
    "compressed_test_data = loaded_r_pca.transform(test_images)\n",
    "print(compressed_test_data.shape)  # This should show (10000, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_centers(data, labels):\n",
    "    centers = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)\n",
    "        centers[label] = np.mean(data[indices], axis=0)\n",
    "    \n",
    "    return centers\n",
    "\n",
    "# Compute centers for each class in MNIST dataset\n",
    "centers = compute_centers(compressed_train_data, train_labels)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the points\n",
    "for i in range(10):  # there are 10 classes in MNIST\n",
    "    indices = np.where(train_labels == i)\n",
    "    ax.scatter(compressed_train_data[indices, 0], compressed_train_data[indices, 1], label=f'Class {i}', alpha=0.5)\n",
    "\n",
    "# Plot the centers\n",
    "for i, center in centers.items():\n",
    "    ax.scatter(center[0], center[1], marker='X', s=200, label=f'Center {i}')\n",
    "\n",
    "ax.set_title('2D PCA of MNIST data')\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Compress test data for evaluation\n",
    "compressed_test_data = r_pca.transform(test_images)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=10000)  # max_iter may need to be increased based on convergence\n",
    "\n",
    "# Train the model on the compressed training data\n",
    "logistic_model.fit(compressed_train_data, train_labels)\n",
    "\n",
    "# Predict on the compressed test data\n",
    "predicted_labels = logistic_model.predict(compressed_test_data)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly select an index from the test set\n",
    "random_idx = random.randint(0, len(test_images) - 1)\n",
    "original_image = test_images[random_idx]\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image.reshape(28, 28), cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Transform and then inverse_transform\n",
    "compressed_image = r_pca.transform(original_image.reshape(1, -1))\n",
    "reconstructed_image = r_pca.inverse_transform(compressed_image)\n",
    "\n",
    "# Display the reconstructed image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed_image.reshape(28, 28), cmap='gray')\n",
    "plt.title('Reconstructed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = r_pca.compute_loss_at_each_layer(train_images)\n",
    "#losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = r_pca.components_at_each_layer()\n",
    "#components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_video(r_pca, data, labels, class_start, class_end, steps=240):\n",
    "    # Get random samples for the start and end classes\n",
    "    start_samples = compressed_train_data[labels == class_start]\n",
    "    end_samples = compressed_train_data[labels == class_end]\n",
    "    \n",
    "    start_point = start_samples[np.random.choice(len(start_samples))]\n",
    "    end_point = end_samples[np.random.choice(len(end_samples))]\n",
    "\n",
    "    # Generate interpolation points\n",
    "    interpolation_points = [start_point + (end_point - start_point) * t/steps for t in range(steps)]\n",
    "    \n",
    "    # Transform these PCA points back to the image space\n",
    "    images = [r_pca.inverse_transform(point.reshape(1, -1)).reshape(28, 28) for point in interpolation_points]\n",
    "    \n",
    "    # Save as video\n",
    "    with imageio.get_writer('transition.mp4', fps=24) as writer:\n",
    "        for img in images:\n",
    "            writer.append_data((img * 255).astype(np.uint8))\n",
    "            \n",
    "    print(\"Video saved as transition.mp4\")\n",
    "\n",
    "# Make sure to load the MNIST data and initialize the RecursivePCA before calling\n",
    "generate_transition_video(r_pca, train_images, train_labels, 2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transition_video(r_pca, data, labels, class_start, class_end, steps=240):\n",
    "    # Compute average PCA representation of each class\n",
    "    centers = compute_centers(compressed_train_data, labels)\n",
    "    \n",
    "    # Generate interpolation points\n",
    "    start_point = centers[class_start]\n",
    "    end_point = centers[class_end]\n",
    "    interpolation_points = [start_point + (end_point - start_point) * t/steps for t in range(steps)]\n",
    "    \n",
    "    # Transform these PCA points back to the image space\n",
    "    images = [r_pca.inverse_transform(point.reshape(1, -1)).reshape(28, 28) for point in interpolation_points]\n",
    "    \n",
    "    # Save as video\n",
    "    with imageio.get_writer('transition.mp4', fps=24) as writer:\n",
    "        for img in images:\n",
    "            writer.append_data((img * 255).astype(np.uint8))\n",
    "            \n",
    "    print(\"Video saved as transition.mp4\")\n",
    "\n",
    "# Make sure to load the MNIST data and initialize the RecursivePCA before calling\n",
    "generate_transition_video(r_pca, train_images, train_labels, 2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635\n",
      "529\n",
      "407\n",
      "291\n",
      "194\n",
      "121\n",
      "71\n",
      "40\n",
      "21\n",
      "10\n",
      "5\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "input_n=700\n",
    "output=2\n",
    "rounds=30\n",
    "delta = input_n - output\n",
    "\n",
    "for i in range (1, rounds):\n",
    "    reduce = (delta*i)/(10+i)\n",
    "    #print(reduce)\n",
    "    delta = delta - reduce\n",
    "    print (round(delta))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
